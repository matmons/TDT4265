{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "First of, we handle boundary condition by implementing zero-pooling. Given that the stride is 1 and as the convolved image should perserve the spatial size we determine the size of zero-padding by\n",
    "\n",
    "$P = (F - 1) / 2 = (3 - 1) / 2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.,   1., -11.,   2.,  13.],\n",
       "       [-10.,   4.,  -8.,  -2.,  18.],\n",
       "       [-14.,   1.,   5.,  -6.,   9.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_data = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 2, 3, 1, 0],\n",
    "        [0, 3, 2, 0, 7, 0, 0],\n",
    "        [0, 0, 6, 1, 1, 4, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "flipped_kernel = np.array([\n",
    "    [1, 0, -1],\n",
    "    [2, 0, -2],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "convolved_output = np.zeros((3,5))\n",
    "\n",
    "for i in range(1, input_data.shape[0]-1):\n",
    "    for j in range(1, input_data.shape[1]-1):\n",
    "        convolved_output[i-1, j-1] = np.sum(np.multiply(kernel, input_data[i-1:i+2, j-1:j+2]))\n",
    "        \n",
    "convolved_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the spatial convolution, the original kernel/filter is flipped. Spatial convolution gives us the following output:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    -2 & 1 & -11 & 2 & 13 \\\\\n",
    "    -10 & 4 & -8 & -2 & 18 \\\\\n",
    "    -14 & 1 & 5 & -6 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "(iii) Max Pooling.\n",
    "\n",
    "Max pooling increases invariance to translation as it disregards the spatial information, but keeps the max value in the area of inspection.\n",
    "\n",
    "Why not (i) - Convolutional layer? From a mere intuitive standpoint, comparing how the output of a convolutional layer and a max pooling layer is calculated, it seems self explanatory that the max pooling is less prone to translational variations. If we look at the filter used in task 1a), a translational shift will also shift the outputs. Max pooling discards spatial information - and \"only cares about\" the max value in the area, meaning that the layer will give the same output even if the input is shifted.\n",
    "\n",
    "Why not (ii) activation function? Does not make sense (?).\n",
    "\n",
    "## Task 1c)\n",
    "Similar to task 1a, we wish to perserve the spatial size through convolution.  Using the same formula, with:\n",
    "$S = 1$\n",
    "and\n",
    "$F = 5$\n",
    "\n",
    "$P = (F - 1) / 2 = (5 - 1) / 2 = 2$\n",
    "\n",
    "## Task 1d)\n",
    "Given equal width and height, we only need to calculate one of the dimensions. Calculating width:\n",
    "\n",
    "$W_{2} = \\frac{W_{1} - F + 2P}{S} + 1$\n",
    "\n",
    "$F = -(W_{2}-1)S + W_{1} + 2P$\n",
    "\n",
    "which gives, with our values:\n",
    "\n",
    "$F = -(504-1)*1 + 512 + 2*0 = 9 $\n",
    "\n",
    "(Height) x (Width) is 9x9. Depth = K = 12.\n",
    "\n",
    "\n",
    "## Task 1e)\n",
    "Pooling layers accepts a volume of size W, H D, and takes hyperparameters F and S as input.\n",
    "\n",
    "The layer produces a volume of size: \n",
    "$W_{2} = \\frac{W_{1} - F}{S} + 1$\n",
    "\n",
    "As we are working with equal width and height, we only have to calculate one of them:\n",
    "\n",
    "$ W_{2} = \\frac{W_{1} - F}{S} + 1 = \\frac{504 - 2}{2} + 1 = 252$\n",
    "\n",
    "The layer produces an output of size: (Height) x (Width) = 252x252. Depth is perserved.\n",
    "\n",
    "## Task 1f)\n",
    "Using the same equation as in task 1d. We now have input, assuming output from the pooling layer in task 1e, of size W,H,D = 252, 252, 12.\n",
    "Again, equal height and width:\n",
    "\n",
    "$W_{2} = \\frac{W_{1} - F + 2P}{S} + 1 = \\frac{252 - 3}{1} + 1 = 250 $ \n",
    "\n",
    "The size of the feature maps: (Height) x (Width) = 250x250\n",
    "\n",
    "## task 1g)\n",
    "390336\n",
    "Legger inn tabellen fra excel n√•r dobbeltsjekket at riktig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![](plots/task2_plot.png)\n",
    "### Task 2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "### Task 3b)\n",
    "### Task 3c)\n",
    "### Task 3d)\n",
    "### Task 3e)\n",
    "### Task 3f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "| Hyperparameters|  |\n",
    "|:-------|----------|\n",
    "|Epochs|10 |\n",
    "|Batch Size|32 |\n",
    "|Learning Rate|0.0005|\n",
    "|Early Stop Count |4 |\n",
    "\n",
    "![](plots/task4_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "Filter activation one and two detect lines, the first activating on vertical lines and the second on horizontal lines. Plots 3 and 5 seem to be activated by larger features, _plot 3_ on the sky and on the zebra, _plot 5_ on the grass, both filter the image in a \"gaussian way\" - blurring out smaller details. Even though _Plot 4_ is dominated by one shade of grey, the zebra is still easily identifiable. Thus, we believe _Plot 4_ activates on contrasts or edges.\n",
    "![](plots/task_4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "To get a better impression of what the activation looks like after the last convolution layer, the zebra is set as a background and the activations are made transparent. Several of the activations are focused around the head and body area of the zebra. What might be of more interest, is the fact the we see very little activation in areas without the zebra (i.e. top right corner).\n",
    "![](plots/task_4c_w_zebra.png)\n",
    "\n",
    "![](plots/task_4c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
