{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "First of, we handle boundary condition by implementing zero-pooling. Given that the stride is 1 and as the convolved image should perserve the spatial size we determine the size of zero-padding by\n",
    "\n",
    "$P = (F - 1) / 2 = (3 - 1) / 2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.,   1., -11.,   2.,  13.],\n",
       "       [-10.,   4.,  -8.,  -2.,  18.],\n",
       "       [-14.,   1.,   5.,  -6.,   9.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_data = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 2, 3, 1, 0],\n",
    "        [0, 3, 2, 0, 7, 0, 0],\n",
    "        [0, 0, 6, 1, 1, 4, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "flipped_kernel = np.array([\n",
    "    [1, 0, -1],\n",
    "    [2, 0, -2],\n",
    "    [1, 0, -1]\n",
    "])\n",
    "\n",
    "convolved_output = np.zeros((3,5))\n",
    "\n",
    "for i in range(1, input_data.shape[0]-1):\n",
    "    for j in range(1, input_data.shape[1]-1):\n",
    "        convolved_output[i-1, j-1] = np.sum(np.multiply(kernel, input_data[i-1:i+2, j-1:j+2]))\n",
    "        \n",
    "convolved_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the spatial convolution, the original kernel/filter is flipped. Spatial convolution gives us the following output:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    -2 & 1 & -11 & 2 & 13 \\\\\n",
    "    -10 & 4 & -8 & -2 & 18 \\\\\n",
    "    -14 & 1 & 5 & -6 & 9 \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "(iii) Max Pooling.\n",
    "\n",
    "Max pooling increases invariance to translation as it disregards the spatial information, but keeps the max value in the area of inspection.\n",
    "\n",
    "Why not (i) - Convolutional layer? From a mere intuitive standpoint, comparing how the output of a convolutional layer and a max pooling layer is calculated, it seems self explanatory that the max pooling is less prone to translational variations. If we look at the filter used in task 1a), a translational shift will also shift the outputs. Max pooling discards spatial information - and \"only cares about\" the max value in the area, meaning that the layer will give the same output even if the input is shifted.\n",
    "\n",
    "Why not (ii) activation function? Does not make sense (?).\n",
    "\n",
    "## Task 1c)\n",
    "Similar to task 1a, we wish to perserve the spatial size through convolution.  Using the same formula, with:\n",
    "$S = 1$\n",
    "and\n",
    "$F = 5$\n",
    "\n",
    "$P = (F - 1) / 2 = (5 - 1) / 2 = 2$\n",
    "\n",
    "## Task 1d)\n",
    "Given equal width and height, we only need to calculate one of the dimensions. Calculating width:\n",
    "\n",
    "$W_{2} = \\frac{W_{1} - F + 2P}{S} + 1$\n",
    "\n",
    "$F = -(W_{2}-1)S + W_{1} + 2P$\n",
    "\n",
    "which gives, with our values:\n",
    "\n",
    "$F = -(504-1)*1 + 512 + 2*0 = 9 $\n",
    "\n",
    "(Height) x (Width) is 9x9. Depth = K = 12.\n",
    "\n",
    "\n",
    "## Task 1e)\n",
    "Pooling layers accepts a volume of size W, H D, and takes hyperparameters F and S as input.\n",
    "\n",
    "The layer produces a volume of size: \n",
    "$W_{2} = \\frac{W_{1} - F}{S} + 1$\n",
    "\n",
    "As we are working with equal width and height, we only have to calculate one of them:\n",
    "\n",
    "$ W_{2} = \\frac{W_{1} - F}{S} + 1 = \\frac{504 - 2}{2} + 1 = 252$\n",
    "\n",
    "The layer produces an output of size: (Height) x (Width) = 252x252. Depth is perserved.\n",
    "\n",
    "## Task 1f)\n",
    "Using the same equation as in task 1d. We now have input, assuming output from the pooling layer in task 1e, of size W,H,D = 252, 252, 12.\n",
    "Again, equal height and width:\n",
    "\n",
    "$W_{2} = \\frac{W_{1} - F + 2P}{S} + 1 = \\frac{252 - 3}{1} + 1 = 250 $ \n",
    "\n",
    "The size of the feature maps: (Height) x (Width) = 250x250\n",
    "\n",
    "## task 1g)\n",
    "390336\n",
    "Legger inn tabellen fra excel når dobbeltsjekket at riktig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![](plots/task2_plot.png)\n",
    "### Task 2b)\n",
    "After 5 epocs early stopping kicks in \n",
    "Final training accuracy: 0.798\n",
    "Final validation accuracy: 0.710\n",
    "Final test accuracy: 0.704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Report your two models. Describe the network architecture (similar to Table 1) and include\n",
    "training details such as optimizer, regularization, learning rate, batch size, weight initialization and\n",
    "other details that are required such that a person reading it can closely replicate your results.\n",
    "\n",
    "#### Network 1\n",
    "Network one is implemented with batch size ..., 10 epochs, initial learning rate ... and early stopping count ..., and the architecture described in the table below. For the dropout layer we choose to increase the dropout rate each time, starting with 20% probability and increasing with 10% for each time, ending up with 50% probability between the fully conected layer. The convolutional layers used a kernel size of 3, and padding and stride of 1, and was implemented with ReLU activation function. Every  pooling layers used a kernel size of 2x2, and stride of 2. \n",
    "The weights was initilazed with ... and we used the SGD optimizer. \n",
    "Further on we tried agumentation to exted our dataset. We used three different transformations: random horizontal flip, .... \n",
    "\n",
    "| Layer | Layer Type  | Number of Hidden Units/Number of Filters    | Activation Function   |\n",
    "|---:|:-------------|:-----------:|:------|\n",
    "| 1 | Conv2d  | 32  | ReLU   | \n",
    "| 1 | BatchNorm2d  | -   | -   | \n",
    "| 2 | Conv2d  | -  | ReLU   | \n",
    "| 2 | BatchNorm2d  | -   | -   | \n",
    "| 2 | MaxPool2d  | -  | -  |  \n",
    "| 3 | Conv2d  | 64 | ReLU   | \n",
    "| 3 | BatchNorm2d  | -   | -   | \n",
    "| 3 | Conv2d  | -   | ReLU   | \n",
    "| 3 | BatchNorm2d  | -   | -   | \n",
    "| 3 | MaxPool2d  | -  | -  | \n",
    "| 4 | Conv2d  | 128  | ReLU   | \n",
    "| 4 | BatchNorm2d  | -   | -   | \n",
    "| 4 | Conv2d  | -   | ReLU   | \n",
    "| 4 | BatchNorm2d  | -   | -   | \n",
    "| 4 | MaxPool2d  | -  | -  | \n",
    "|   | Flatten  | -  | -  | \n",
    "| 6 | Fully-Connected  | 64   | ReLU  | \n",
    "| 6 | BatchNorm1d  | -   | -   | \n",
    "| 7 | Fully-Connected  | 10   | Softmax   | \n",
    "\n",
    "#### Network 2\n",
    "For the second Network we used the same hyperparameters as for network 1. We changed the architecture and choosed to apply one more layer with trainable layer compared to the network in task 2. The architecture described in the table below. For this network we did not implement any regularization, but choose to replace all the ReLU activation functions with LeakyReLU.  The convolutional layers used a kernel size of 5, padding of 2 and stride of 1. Every  pooling layers used a kernel size of 2x2, and stride of 2. \n",
    "The weights was initilazed with ... and we used the Adam optimizer. \n",
    "\n",
    "| Layer | Layer Type  | Number of Hidden Units/Number of Filters    | Activation Function   |\n",
    "|---:|:-------------|:-----------:|:------|\n",
    "| 1 | Conv2d  | 32  | LeakyReLU   | \n",
    "| 1 | MaxPool2d  | -  | -  | \n",
    "| 1 | Dropout  | -   | -   |\n",
    "| 2 | Conv2d  | 64 | LeakyReLU   | \n",
    "| 2 | MaxPool2d  | -  | -  | \n",
    "| 2 | Dropout  | -   | -   |\n",
    "| 3 | Conv2d  | 128  | LeakyReLU   | \n",
    "| 3 | MaxPool2d  | -  | -  | \n",
    "| 3 | Dropout  | -   | -   |\n",
    "| 4 | Conv2d  | 256  | LeakyReLU   | \n",
    "| 4 | MaxPool2d  | -  | -  | \n",
    "| 4 | Dropout  | -   | -   |\n",
    "|   | Flatten  | -  | -  | \n",
    "| 5 | Fully-Connected  | 64   | LeakyReLU  |\n",
    "| 5 | Dropout  | -   | -   |\n",
    "| 6 | Fully-Connected  | 10   | Softmax   | \n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "Final statistics from the two models. \n",
    "\n",
    "| Network  | 1 | 2  |\n",
    "|--------:|:---:|:---:|\n",
    "| Training Loss | 0.48 | 0.45  |  \n",
    "| Training Accuracy | 0.834 | 0.854  |  \n",
    "| Validation Accuracy | 0.800 | 0.770  |  \n",
    "| Test Accuracy | 0.827 | 0.771  |  \n",
    "\n",
    "\n",
    "#### Plots from best model: Network 1\n",
    "\n",
    "![](plots/task3_network1.1_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "### Task 3c)\n",
    "Discuss briefly what methods you found useful, and answer the following questions.\n",
    "• Which method did you see the improvement with? Why do you think the network improved\n",
    "with this method?\n",
    "• Which method did not work? Why did it not work?\n",
    "Task2 test 0.704 \n",
    "\n",
    "#### Agumentation\n",
    "horizontal flip\n",
    "Epoch: 9, Batches per seconds: 34.07, Global step:   6669, Train Accuracy: 0.862, Validation Loss: 0.68, Validation Accuracy: 0.772, Test Loss: 0.71, Test Accuracy: 0.769\n",
    "rotation 5\n",
    "Epoch: 9, Batches per seconds: 30.81, Global step:   7020, Train Accuracy: 0.877, Validation Loss: 0.88, Validation Accuracy: 0.726, Test Loss: 0.83, Test Accuracy: 0.739\n",
    "crop\n",
    "Epoch: 9, Batches per seconds: 30.30, Global step:   7020, Train Accuracy: 0.777, Validation Loss: 0.80, Validation Accuracy: 0.729, Test Loss: 0.75, Test Accuracy: 0.747\n",
    "all\n",
    "Epoch: 9, Batches per seconds: 23.47, Global step:   7020, Train Accuracy: 0.751, Validation Loss: 0.80, Validation Accuracy: 0.720, Test Loss: 0.72, Test Accuracy: 0.752\n",
    "\n",
    "#### Regularization by Dropout\n",
    "By applying dropout we increase the test accuracy with .. % , and achieve …% accuracy. We tried to use both p=2 for eacy dropout layer, and a increase of 10% for each dropout, therefore a harder dropout. A consistent dropout percentage of 2 gave the best results. Dropout regularizaise the network, and makes it less prone to overfitting. It does this by reducing its parameters by randomly drop out nodes along with their connection. Thereby the remaining nodes must addapt. \n",
    "\n",
    "#### Batch normalization\n",
    "We can see with batch normalization the network train faster, we acheive the same accuracy earlier in the training. Compared to the model in task 2, that acheived a test accuracy at 70.4% with 5 (har gått over dette beyond, tatt igjen) epochs the same accuracy we achive by 2 epochs with normalization and end up with a final test accuracy at ---%. This is an improvement by … This is because normalization normalize the activation of the previous layer at each batch and therefore addresses the problem of internal covariance shift, and with this it stabilize the learning and accelerate the learning process. It also acts as a regularizer. \n",
    "\n",
    "#### Kernel size 3x3\n",
    "Epoch: 7, Batches per seconds: 39.00, Global step:   5616, Train Accuracy: 0.814, Validation Loss: 0.82, Validation Accuracy: 0.716, Test Loss: 0.88, Test Accuracy: 0.705\n",
    "\n",
    "#### LeakyRelu activation\n",
    "0.1:\n",
    "Epoch: 5, Batches per seconds: 39.56, Global step:   4212, Train Accuracy: 0.800, Validation Loss: 0.89, Validation Accuracy: 0.701, Test Loss: 0.92, Test Accuracy: 0.698\n",
    "0.01:\n",
    "Epoch: 9, Batches per seconds: 38.41, Global step:   6669, Train Accuracy: 0.937, Validation Loss: 0.89, Validation Accuracy: 0.752, Test Loss: 0.91, Test Accuracy: 0.743\n",
    "\n",
    "#### Adam optimizer\n",
    "Epoch: 1, Batches per seconds: 39.57, Global step:   1404, Train Accuracy: 0.100, Validation Loss: 2.30, Validation Accuracy: 0.100, Test Loss: 2.30, Test Accuracy: 0.100\n",
    "kan vel ikke være riktig???\n",
    "- deeper network \n",
    "one more\n",
    "Epoch: 6, Batches per seconds: 36.48, Global step:   4914, Train Accuracy: 0.885, Validation Loss: 0.76, Validation Accuracy: 0.743, Test Loss: 0.80, Test Accuracy: 0.730\n",
    "two more\n",
    "Epoch: 9, Batches per seconds: 34.49, Global step:   7020, Train Accuracy: 0.928, Validation Loss: 0.84, Validation Accuracy: 0.757, Test Loss: 0.83, Test Accuracy: 0.751\n",
    "- different architecture\n",
    "Epoch: 5, Batches per seconds: 31.28, Global step:   3861, Train Loss: 0.20, Train Accuracy: 0.933, Validation Loss: 0.65, Validation Accuracy: 0.797, Test Loss: 0.66, Test Accuracy: 0.796\n",
    "\n",
    "### Task 3d)\n",
    "Plot of validation loss with and without architecture ..., that gave the largest amount of improvement among the different techniques described in 3c). \n",
    "\n",
    "![](plots/task3_best_improvement_loss_plot.png)\n",
    "\n",
    "### Task 3e)\n",
    "We decided to improve Network 1 even more by adding dropout after max pooling, and remove the crop and rotation transformation. This is based on the results from task 3c). This resulted in even better test accuracy, and final test accuracy is 0.844. \n",
    "\n",
    "![](plots/task3_network2_80_plot.png)\n",
    "\n",
    "### Task 3f)\n",
    "By observing the loss plot, the validation loss and training loss is quite similar, that suggest that the model is not overfitting. When analyzing the accuracy plot, we observe that the train accuracy is higher than the validation and test accuracy. Taking both plots into account it seems like the model is underfitting to some degree. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "| Hyperparameters|  |\n",
    "|:-------|----------|\n",
    "|Epochs|10 |\n",
    "|Batch Size|32 |\n",
    "|Learning Rate|0.0005|\n",
    "|Early Stop Count |4 |\n",
    "\n",
    "![](plots/task4_plot.png)\n",
    "After 2 epochs early stopping criteria is met and we end up with a final test accuracy at 0.889. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "Filter activation one and two detect lines, the first activating on vertical lines and the second on horizontal lines. Plots 3 and 5 seem to be activated by larger features, _plot 3_ on the sky and on the zebra, _plot 5_ on the grass, both filter the image in a \"gaussian way\" - blurring out smaller details. Even though _Plot 4_ is dominated by one shade of grey, the zebra is still easily identifiable. Thus, we believe _Plot 4_ activates on contrasts or edges.\n",
    "![](plots/task_4b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "To get a better impression of what the activation looks like after the last convolution layer, the zebra is set as a background and the activations are made transparent. Several of the activations are focused around the head and body area of the zebra. What might be of more interest, is the fact the we see very little activation in areas without the zebra (i.e. top right corner).\n",
    "![](plots/task_4c_w_zebra.png)\n",
    "\n",
    "![](plots/task_4c.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
