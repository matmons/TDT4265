{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We have from equation 3  \n",
    "\\begin{equation}\n",
    "    w_{j,i} = w_{j,i} - \\delta _{j}x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "Where\n",
    "\\begin{equation}\n",
    "    \\delta _{j} = \\frac{\\partial C}{\\partial z_{j}} = \\displaystyle\\sum_{k}  \\frac{\\partial C}{\\partial z_{k}} \\frac{\\partial z_{k}}{\\partial a_{j}} \\frac{\\partial a_{j}}{\\partial z_{j}}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "and from equation 2\n",
    "\\begin{equation}\n",
    "    w_{j,i} = w_{j,i} - \\alpha \\frac{\\partial C}{\\partial w_{j,i}}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "By using the chain rule we have that\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C}{\\partial w_{j,i}} = \\displaystyle\\sum_{k} \\frac{\\partial z_{j}}{\\partial w_{j,i}}\\frac{\\partial a_{j}}{\\partial z_{j}}\\frac{\\partial z_{k}}{\\partial a_{j}}\\frac{\\partial C}{\\partial z_{k}}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "Compare terms in (3) and (5), we get that\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C}{\\partial w_{j,i}} = \\displaystyle\\sum_{k} \\frac{\\partial z_{j}}{\\partial w_{j,i}}\\delta _{j}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "where\n",
    "\\begin{equation}\n",
    "    z_{j} = w_{j,i} x_{i}  =>  \\frac{\\partial z_j}{\\partial w_{j,i}} = x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "that gives \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C}{\\partial w_{j,i}} = \\delta _{j}x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "and therefore\n",
    "\\vspace{5mm}\n",
    "\\begin{equation}\n",
    "    w_{j,i} =  w_{j,i} - \\alpha \\delta_{j} x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{10mm}\n",
    "To show that \n",
    "\\vspace{5mm}\n",
    "\\begin{equation}\n",
    "    \\delta_{j} = f'(z_{j}) \\displaystyle\\sum_{k} w_{k,j} \\delta_{k} \n",
    "\\end{equation}\n",
    "\n",
    "We look at the terms on the rigth side \n",
    "\\vspace{5mm}\n",
    "where we have that\n",
    "\\begin{equation}\n",
    "    a_{j} = f(z_{j}) = >  \\frac{\\partial a_{j}}{\\partial z_{j}} = f'(z_{j}) \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\delta_{k} = \\frac{\\partial C}{\\partial z_{k}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    z_{k} = w_{k,j}a_{j} => \\frac{\\partial z_{k}}{\\partial a_{j}} = w_{k,j}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "Setting in (11), (12) and (13) in (10), we get that\n",
    "\\begin{equation}\n",
    "    \\delta_{j} = f'(z_{j}) \\displaystyle\\sum_{k} w_{k,j} \\delta_{k} = \\frac{\\partial a_{j}}{\\partial z_{j}}\\displaystyle\\sum_{k}  \\frac{\\partial z_{k}}{\\partial a_{j}}\\frac{\\partial C}{\\partial z_{k}}\n",
    "\\end{equation}\n",
    "\n",
    "\\vspace{5mm}\n",
    "By comparing (14) and (3) we se that we have the same function, by setting the term \n",
    "\\begin{equation}\n",
    "    \\frac{\\partial a_{j}}{\\partial z_{j}}\n",
    "\\end{equation}\n",
    "outside the sum, that we can do because it does not depend on k. Then we have shown that (10) is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "Notation: vectors are shown with an arrow above the symbol and matrix is shown by using bold capital letter. \n",
    "\n",
    "We have the update rule for the output layer\n",
    "\\begin{equation}\n",
    "    w_{k,j} =  w_{k,j} - \\alpha \\frac{\\partial C}{\\partial w_{k,j}} = w_{k,j}- \\alpha\\delta_{k}a_{j}\n",
    "\\end{equation}\n",
    "\n",
    "and for the hidden layer\n",
    "\\begin{equation}\n",
    "    w_{j,i} =  w_{j,i} - \\alpha \\frac{\\partial C}{\\partial w_{j,i}} = w_{j,i}- \\alpha\\delta_{j}x_{i}=w_{j,i}-\\alpha f'(z_{j}) \\displaystyle\\sum_{k} w_{k,j} \\delta_{k} x_{i}\n",
    "\\end{equation}\n",
    "\n",
    "On vector form for the outputlayer  (16)\n",
    "\\begin{equation}\n",
    "    \\mathbf{W_{k,j}} = \\mathbf{W_{k,j}} - \\alpha\\overrightarrow{\\delta_{k}}\\overrightarrow{a_{j}}^T\n",
    "\\end{equation}\n",
    "\n",
    "For the hidden layer, (17), wee first look at \n",
    "\\begin{equation}\n",
    "    \\displaystyle\\sum_{k} w_{k,j} \\delta_{k} = \\mathbf{W_{k,j}} \\overrightarrow{\\delta_{k}}\n",
    "\\end{equation}\n",
    "where W is an j x k matrix, and the dot product will be an colon vector of size j \n",
    "\n",
    "This gives us \n",
    "\\begin{equation}\n",
    "    \\mathbf{ W_{j,i} } =\n",
    "    \\mathbf{ W_{j,i} } -\n",
    "    \\alpha \\mathbf{\\overrightarrow{f}}'(\\overrightarrow{z}_{j})\\mathbf{W_{k,j}} \\overrightarrow{\\delta_{k}}\\overrightarrow{x}_{i}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "    \\overrightarrow{z}_{j} = \\overrightarrow{x}_{j}\\mathbf{W}_{j,i}^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "$ Parameters = w_{ji} + w_{kj} = 785*64+64*10 = 50880$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a general level, implementing the various \"tricks of the trade\" improves the accuracy and convergence speed. As the improved sigmoid depends on zero-centered input, improved weight initialization was implemented first. The network with improved weights achieves a much higher validation accuracy in a shorter time (in epochs) compared to its \"default\" counterpart. \n",
    "\n",
    "Further improvements of the network (improved sigmoid & momentum) performs similarly. Altough similar to each other, these improvements outperform the other two models in greater convergence speed and accuracy.\n",
    "\n",
    "Initial thoughts suggest that as all networks are based on the same topology, networks will face similar challenges concerning overfitting/generalization. However, greater convergence speeds results in earlier early-stopping - which does reduce overfitting. \n",
    "\n",
    "Furthermore, it does seem like - from our plots - that the network with only improved weight initialization could, in another training, have been stopped at an earlier time. The validation accuracy flattens, but minor validation accuracy improvements are continously made. This is only a minor case and we do not see it as significant.\n",
    "![](task3_tricks_of_the_trade.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a & 4b)\n",
    "\n",
    "By reducing the number of neurons in the hidden layer to 32, we observe that the validation accuracy worsen. We now get at validation accuracy at 0.9497, compared to task 3 with 64 neurons with a accuracy at 0.9615. We also observe a slower learning speed, it takes longer for the loss to converge. With fewer neurons we would expect the opposite, that the training would get faster with fewer weights to update. The level of complexity and decrease of validation and training accuracy to this model suggest underfitting is causing the reduction in performance in this case. \n",
    "\n",
    "On the other hand, doubling the number of neurons to 128 neurons in the hidden layer increased validation accuracy to 0.9669. We can also see that training loss decreased thus improved, and had a higher learning speed. We would expect the model with double the neuron to overfit, because of its increase in complexity. By observing the training accuracy it is quite similar to our model with 64 neurons, but the validation accuracy is higher for the higher number of neurons. This suggest that the model with 128 neurons generally performs better. \n",
    "\n",
    "From the plotts below we see that with more neurons in the hidden layer the validation increase, and get better convergence- and training speed. The down side with more neurons in the hidden layer is that our network is more complex, that makes the model more prone to overfitting. The complexity of the model will be a balance between being too complex and not enough complexity, since lack of complexity can cause underfitting. \n",
    "![](task4_network_topology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "$ Parameters = 785*59+59*59 + 59*10 = 50386 $. This gives us 59 hidden units in each hidden layer. Compoared to task 3, this is 494 fewer parameter.\n",
    "\n",
    "Adding a layer to the model increase its complexity, since more layers means less linear function. On the other hand we keep the number of parameter almost the same, and therefore not adding more complexity in terms of parameter. \n",
    "\n",
    "By observing the plots below, there is a slight difference, between two hidden layers with 59 neurons and one hidden layer with 64 neurons. For the model with two hidden layers we see that early stopping is quicking in earlier than with one layer. From our model from task 3 we observe that validation loss first drops before it starts to increase agian, which imply overfitting of the model. At around the same point for the model with two hidden layers early stopping kicks in. On one hand it can seem like adding a layer in this case helps prevent overfitting, but on the other hand the validation loss i higher for the model with two layers, which is not preferable. It does not seem like adding a layer to the model improves performance in this case. Here it could be interesting to add regularization as done in the privious assignement to se the effect it would have on preventing overfitting and if it would increase performance in this case. \n",
    "\n",
    "![](task4d_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "The performance decrease significantly with ten hidden layers, as earlier stated higher complexity to the model makes the model prone to overfitting. Adding 10 hidden layers adds a lot of complexity, hence the decrease in performance due to overfitting. Her we also see that the training accuracy decrease, and it seems that our network is not able to train the network with the high level of complexity. It seams like adding many layers makes it harder for the model to train. Which makes sense due to more weigths to update.\n",
    "\n",
    "\n",
    "![](task4e_train_loss.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
